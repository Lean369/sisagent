import os
from typing import Annotated, TypedDict, List
import operator
from dotenv import load_dotenv
from loguru import logger
import sys
import json
import threading

# --- Imports de IA ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.messages import ToolMessage

# --- Imports de Grafo y DB ---
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from psycopg_pool import ConnectionPool
from utilities import es_horario_laboral, obtener_nombres_dias, obtener_configuraciones, gestionar_expiracion_sesion

# --- Imports de Herramientas ---
from crm_tools import trigger_booking_tool, consultar_stock, ver_menu
from hitl_tools import solicitar_atencion_humana
from analytics import registrar_evento
import time

# Cargar .env
load_dotenv(override=True)

# Configurar el saver de Postgres para LangGraph
DB_HOST = os.getenv('DB_HOST', 'localhost')
DB_NAME = os.getenv('DB_NAME_AGENT', 'checkpointer_db')
DB_USER = os.getenv('DB_USER', 'sisbot_user')
DB_PASSWORD = os.getenv('DB_PASSWORD', 'postgres_password')
DB_PORT = os.getenv('DB_PORT', '5432')
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your_openai_key")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "your_google_key")
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "your_groq_key")

# Configuraci√≥n √∫nica de Logger con Loguru
try:
    # Intentamos remover el default solo si es la primera vez
    logger.remove(0) 
except ValueError:
    pass # Ya estaba removido

# 1. Salida en Consola (Colorida y legible)
logger.add(
    sys.stderr,
    level="DEBUG",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)

# 2. Archivo con ROTACI√ìN (10 MB) y COMPRESI√ìN (zip)
# Esto guardar√° en la carpeta 'logs', rotar√° al llegar a 10MB y borrar√° logs de m√°s de 20 d√≠as.
logger.add(
    "logs/sisagent_verbose.log",
    rotation="10 MB",
    retention="20 days",
    compression="zip",
    level="DEBUG",
    encoding="utf-8"
)

logger.info("üöÄ Iniciando la Agente AI...")


# ==============================================================================
# REGISTRO DE HERRAMIENTAS
# ==============================================================================
# Mapeo de nombres de herramientas a objetos tool
TOOLS_REGISTRY = {
    "consultar_stock": consultar_stock,
    "ver_menu": ver_menu,
    "trigger_booking_tool": trigger_booking_tool,
    "solicitar_atencion_humana": solicitar_atencion_humana
}

# ==============================================================================
# 1. SETUP GLOBAL (MODELOS Y DB)
# ==============================================================================
# Patron factory para obtener el modelo LLM seg√∫n configuraci√≥n
def get_llm_model(provider_override=None):
    """Retorna el modelo LLM seg√∫n la configuraci√≥n
   
    Args:
        provider_override: Si se especifica, usa este provider en lugar del configurado
    """
    try:
        provider = provider_override or os.getenv("LLM_PROVIDER", "google").lower()
        
        if provider == "openai":
            OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            logger.info("Usando modelo OpenAI:" + OPENAI_MODEL)
            return ChatOpenAI(model=OPENAI_MODEL, temperature=0, max_retries=2)
        
        elif provider == "groq":
            GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")
            logger.info("Usando modelo Groq " + GROQ_MODEL)
            return ChatGroq(model=GROQ_MODEL, temperature=0, max_retries=2)

        elif provider == "gemini":
            GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")
            logger.info("Usando modelo Google Gemini " + GEMINI_MODEL)
            return ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0, max_retries=2)

        else:
            raise ValueError(f"Proveedor LLM desconocido: {provider}")

    except Exception as e:
        logger.exception(f"üî¥ Error al inicializar el modelo LLM ({provider}): {e}")


LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini").lower()
LLM_PROVIDER_FALLBACK = os.getenv("LLM_PROVIDER_FALLBACK", "openai").lower()

llm_primary = get_llm_model(LLM_PROVIDER)
llm_backup = get_llm_model(LLM_PROVIDER_FALLBACK)

logger.info(f"LLM Provider configurado: {LLM_PROVIDER}")
logger.info(f"LLM Provider fallback: {LLM_PROVIDER_FALLBACK}")

DB_URI = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# Configuraci√≥n del pool de conexiones a Postgres para el checkpointer de LangGraph
# Soporta hasta 20 conexiones concurrentes (20 usuarios simult√°neos). Ajustar seg√∫n necesidades y recursos.
pool = ConnectionPool(conninfo=DB_URI, min_size=1, max_size=20, kwargs={"autocommit": True})

with pool.connection() as conn:
    checkpointer_temp = PostgresSaver(conn)
    checkpointer_temp.setup()


def _lanzar_metricas_background(pool, response_msg, thread_id, latency_ms, isLlmPrimary=True):
    """Lanza el registro de m√©tricas en un hilo independiente para no bloquear."""
    hilo = threading.Thread(
        target=registrar_evento,
        args=(pool, response_msg, thread_id, latency_ms, isLlmPrimary),
        daemon=False # False asegura que se guarde aunque el request principal termine
    )
    hilo.start()

# ==============================================================================
# 2. DEFINICI√ìN DEL GRAFO MULTI-TENANT
# ==============================================================================
class State(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]


# Prompting Din√°mico "Zero-Storage". Se inyecta el SystemMessage al vuelo en la variable mensajes_entrada.
# Recibe el estado anteror de la conversaci√≥n (state) y la configuraci√≥n del negocio (config) para decidir qu√© prompt, herramientas y puede devuelve un estado nuevo / actualizado
# NO lo agrego al state para ahorrar tokens en la DB.
# Si se cambia la configuraci√≥n del negocio, se aplica inmediatamente.
def nodo_chatbot(state: State, config: RunnableConfig):
    # 1. Recuperar Configuraci√≥n desde par√°metro config (que viene de app.py)
    configurable = config.get("configurable", {})
    business_id = configurable.get("business_id", "default")
    nombre_cliente = configurable.get("client_name", "Cliente")
    thread_id = configurable.get("thread_id", "unknown_thread")
    
    # 2. Recuperar Configuraci√≥n desde archivo config_negocios.json
    config_actual = obtener_configuraciones() 
    info_negocio = config_actual.get(business_id)
    if not info_negocio:
        logger.error(f"üî¥ No se encontr√≥ configuraci√≥n para business_id: {business_id}.")
        return {"messages": [AIMessage(content="No se encontr√≥ la configuraci√≥n del negocio. Por favor, contacta al soporte.")]}
    
    logger.info(f"üíº Negocio: {business_id} | Thread: {thread_id}")

    # 3. Verificar horario laboral
    en_horario, mensaje_fuera_horario = es_horario_laboral(info_negocio)
    if not en_horario:
        logger.info(f"‚è∞ Fuera de horario laboral para {business_id}. Respondiendo con mensaje de fuera de servicio.")
        return {"messages": [AIMessage(content=mensaje_fuera_horario)]}

    # ---------------------HITL------------------------------------
    mensajes_historia = state["messages"]
    bot_pausado = False
    
    # Escaneamos hacia atr√°s para ver el estado actual
    for msg in reversed(mensajes_historia):
        # 1. Si encontramos una se√±al de reactivaci√≥n, el bot est√° ACTIVO
        if "BOT_REACTIVADO" in str(msg.content):
            bot_pausado = False
            break
            
        # 2. Si encontramos la se√±al de derivaci√≥n, el bot est√° PAUSADO
        # Buscamos el string exacto que retorna tu tool 'solicitar_atencion_humana'
        if isinstance(msg, ToolMessage) and "DERIVACION_EXITOSA_SILENCIO" in str(msg.content):
            bot_pausado = True
            break
    
    if bot_pausado:
        logger.warning(f"‚õî Bot pausado para {business_id} (Derivaci√≥n activa). Ignorando mensaje.")
        # Retornamos una lista vac√≠a o un mensaje nulo para detener el grafo
        # Dependiendo de tu versi√≥n de LangGraph, esto puede requerir devolver un dict vac√≠o
        # o un mensaje especial.
        return {"messages": []} 
    # ---------------------------------------------------------

    # 4. Convertir nombres de tools a objetos tool
    tools_nombres = info_negocio.get('tools_habilitadas', []) if info_negocio else []
    mis_tools = []
    for tool_nombre in tools_nombres:
        if isinstance(tool_nombre, str) and tool_nombre in TOOLS_REGISTRY:
            mis_tools.append(TOOLS_REGISTRY[tool_nombre])
        elif not isinstance(tool_nombre, str):
            mis_tools.append(tool_nombre) 

    prompt_sistema = info_negocio['system_prompt'] if info_negocio else "Eres un asistente √∫til."
    if isinstance(prompt_sistema, list):
        prompt_sistema_unido = "\n".join(prompt_sistema)  # Une los strings con saltos de l√≠nea
    else:
        prompt_sistema_unido = prompt_sistema

    if len(nombre_cliente) > 3:
        prompt_final = (
        f"{prompt_sistema_unido}\n"
        f"DATOS DE CONTEXTO:\n"
        f"- Est√°s hablando con: {nombre_cliente}.\n"
    )
    else:
        prompt_final = prompt_sistema_unido

    #logger.debug(f"üìù Prompt final para {business_id}:\n{prompt_final}")

    # 5. VINCULACI√ìN DIN√ÅMICA (Aqu√≠ ocurre la magia ‚ú®)
    if mis_tools:
        # Creamos una instancia temporal del LLM que solo conoce estas tools
        llm_actual = llm_primary.bind_tools(mis_tools)
        llm_backup_actual = llm_backup.bind_tools(mis_tools)
        logger.info(f"üîß Vinculadas {len(mis_tools)} herramientas para {business_id}")
        for tool in mis_tools:
            logger.info(f"Tool vinculada: {tool.name}")
    else:
        # Si no hay tools, usamos el modelo base sin capacidades extra
        llm_actual = llm_primary
        llm_backup_actual = llm_backup
        logger.info(f"‚ÑπÔ∏è No hay herramientas vinculadas para {business_id}")
    
    # 6. Construir mensajes (System + Historia)
    mensajes_entrada = [SystemMessage(content=prompt_final)] + state["messages"]

    logger.info(f"Ejecutando LLM para thread: {thread_id}")
    # ‚è±Ô∏è INICIO CRON√ìMETRO (Solo para el LLM)
    start_time = time.time()
    
    try:
        # 7. Invocaci√≥n al LLM con manejo de errores interno (fallback a modelo backup)
        response_msg = llm_actual.invoke(mensajes_entrada)
        
        # ‚è±Ô∏è C√ÅLCULO DE TIEMPO
        latency_ms = int((time.time() - start_time) * 1000)

        logger.success(f"Respuesta de llm_actual para {thread_id}: {response_msg.content[:200]}...")

        _lanzar_metricas_background(pool, response_msg, thread_id, latency_ms, isLlmPrimary=True)

        
        # 6. RETORNO CORRECTO: Debe ser un dict con la clave 'messages'
        # LangGraph tomar√° esto y har√° un append a la lista de mensajes en la DB.
        return {"messages": [response_msg]}

    except Exception as e:
        #logger.exception(f"‚ö†Ô∏è Fallo LLM primario ({e}). Cambiando a respaldo...")
        logger.warning(f"‚ö†Ô∏è Fallo LLM primario para {thread_id} ({e}). Cambiando a respaldo...")
        try:
            response_msg = llm_backup_actual.invoke(mensajes_entrada)

            # ‚è±Ô∏è C√ÅLCULO DE TIEMPO
            latency_ms = int((time.time() - start_time) * 1000)

            logger.success(f"Respuesta de llm_backup_actual para {thread_id}: {response_msg.content[:200]}...")

            _lanzar_metricas_background(pool, response_msg, thread_id, latency_ms, isLlmPrimary=False)
                
            return {"messages": [response_msg]}

        except Exception as e2:
            logger.error(f"üî∫ Fallo total para {thread_id}: {e2}")
            # Devolvemos un mensaje de error encapsulado en AIMessage para no romper el flujo
            return {"messages": [AIMessage(content="Lo siento, tengo un problema t√©cnico temporal.")]}


# ==============================================================================
# 3. DEFINICI√ìN DE HERRAMIENTAS Y NODOS DE EJECUCI√ìN
# ==============================================================================

def obtener_todas_las_tools() -> dict:
    """
    Retorna todas las herramientas √∫nicas definidas en TOOLS_REGISTRY.
    """
    try:
        # Recolectar todos los nombres de tools_habilitadas de cada negocio
        avalilable_tools = set()
        config_actual = obtener_configuraciones() 
        for negocio_conf in config_actual.values():
            if not isinstance(negocio_conf, dict):
                continue
            tools_list = negocio_conf.get("tools_habilitadas", [])
            if isinstance(tools_list, list):
                for t in tools_list:
                    if isinstance(t, str):
                        avalilable_tools.add(t)

        avalilable_tools = sorted(list(avalilable_tools))
        logger.info(f"Herramientas configuradas en JSON (nombres √∫nicos): {avalilable_tools}")

        if avalilable_tools:
            herramientas_filtradas = []
            for tool_nombre in avalilable_tools:
                if tool_nombre in TOOLS_REGISTRY:
                    herramientas_filtradas.append(TOOLS_REGISTRY[tool_nombre])

            logger.info(f"Total de herramientas √∫nicas cargadas desde JSON: {len(herramientas_filtradas)}")
            for tool in herramientas_filtradas:
                logger.info(f"Tool registered from JSON: {tool.name}")
            return herramientas_filtradas
        else:
            logger.warning("‚ö†Ô∏è No hay herramientas espec√≠ficas en JSON 'config_negocios.json'")
            return []    
    
    except Exception as e:
        logger.exception("üî¥ Error cargando herramientas desde TOOLS_REGISTRY")
        return []


tool_node = ToolNode(obtener_todas_las_tools(), handle_tool_errors=True)

# Construcci√≥n del grafo. Se define la estructura del grafo una vez y es inmutable durante la ejecuci√≥n.
workflow_builder = StateGraph(State)

workflow_builder.add_node("chatbot", nodo_chatbot)
workflow_builder.add_node("tools", tool_node) # Nodo ¬¥tool_node¬¥ es gen√©rico de ejecuci√≥n

workflow_builder.set_entry_point("chatbot")

# L√≥gica condicional (creaci√≥n de aristas): Si el chatbot pide tool -> va a 'tools', si no -> END
workflow_builder.add_conditional_edges(
    "chatbot",
    tools_condition
)

workflow_builder.add_edge("tools", "chatbot") # Volver al chatbot con el resultado

# ==============================================================================
# 4. FUNCI√ìN DE PROCESAMIENTO con LLM y Memoria Separada
# ==============================================================================
def procesar_mensaje(mensaje_usuario: str, config: dict) -> dict:
    try:
        if not mensaje_usuario: return {"status": "ERROR", "response": "Mensaje vac√≠o"}

        conf_data = config.get('configurable', {})
        thread_id = conf_data.get('thread_id', 'unknown')
        business_id = conf_data.get('business_id', 'unknown')
        
        # ---------------------Verificaci√≥n de sesi√≥n expirada------------------------------------
        config_actual = obtener_configuraciones() 
        info_negocio = config_actual.get(business_id)
        ttl_minutos = info_negocio.get("ttl_sesion_minutos", 60)
        logger.info(f"Procesando msg. thread={thread_id}, business={business_id}, ttl_sesion={ttl_minutos}min")

        # üßπ --- NUEVA L√ìGICA DE LIMPIEZA ---
        # Si la sesi√≥n expir√≥, esto borra la DB y el bot arranca de cero.
        sesion_reseteada = gestionar_expiracion_sesion(pool, thread_id, ttl_minutos)
        
        if sesion_reseteada:
            logger.info(f"üßπ Sesi√≥n reiniciada para {thread_id} por inactividad.")

        with pool.connection() as conn:
            checkpointer = PostgresSaver(conn)
            app = workflow_builder.compile(checkpointer=checkpointer)        
            
            inputs = {"messages": [HumanMessage(content=mensaje_usuario)]}
            
            # Ejecuci√≥n
            result = app.invoke(inputs, config=config)

            mensajes = result.get("messages", [])

            # 1. Validaci√≥n b√°sica de mensajes vac√≠os
            if not mensajes: 
                return {"status": "PAUSED", "response": ""}

            ultimo_mensaje = mensajes[-1]
            contenido_final = ultimo_mensaje.content

            # --- CORRECCI√ìN AQU√ç ---
            # En lugar de mirar solo el √∫ltimo mensaje, miramos si la se√±al apareci√≥ 
            # en CUALQUIER parte de la ejecuci√≥n reciente (en la Tool o en el AI).
            
            se_debe_silenciar = False
            
            for msg in reversed(mensajes):
                contenido = str(msg.content)
                
                # 1. Si encontramos la se√±al de REACTIVACI√ìN primero, el bot est√° VIVO.
                # Rompemos el ciclo inmediatamente porque lo que pas√≥ antes ya no importa.
                if "BOT_REACTIVADO" in contenido:
                    se_debe_silenciar = False
                    logger.info(f"üü¢ Se√±al de reactivaci√≥n encontrada para {thread_id}. Permitiendo respuesta.")
                    break 
                
                # 2. Si encontramos la se√±al de SILENCIO primero, el bot sigue PAUSADO.
                if "DERIVACION_EXITOSA_SILENCIO" in contenido:
                    se_debe_silenciar = True
                    logger.warning(f"‚õî Se√±al de silencio encontrada (y es la m√°s reciente) para {thread_id}.")
                    break

            if se_debe_silenciar:
                return {"status": "PAUSED", "response": ""}
            
            # 4. Retorno normal
            return {
                "status": "COMPLETED",
                "response": contenido_final
            }

    except Exception as e:
        logger.exception(f"üî¥ Error cr√≠tico en procesar_mensaje: {e}")
        return {
            "status": "ERROR",
            "response": "Error interno. Por favor, intenta nuevamente m√°s tarde."
        }